# -*- coding: utf-8 -*-
"""violence detection dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/mustafaalali7/violence-detection-dataset.88f22f7d-33d6-4acd-ae61-93b90de07331.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250409/auto/storage/goog4_request%26X-Goog-Date%3D20250409T164747Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D296bfe0f597921ea4e4ce8b52302140ee46f2739b1a445680906e2d8dcfcfba5e81e9711edcbd505fbd7d079583c6a1d457d573273feb2ee54a4a57f44ba513b47f932aa34ff0daf8456ee37089bd998d0e58f15b02db3d2a15cef03bdcb2fe21a63a33d809730231aaa1fefe3f636c26c08c59cd0f4c655c6fd604c8bb6a78ef5617cef49b7ac6e4c90adf09e33c9320aeb543d4e208c9103de6e262a5e56b23407f8316748591db6b900946321886c95ec4c8f4a23bdf62defa428b30b31d7d5e4e4d4994b667ff00ef55b95a64abf24d2721232fb52686f68794896cc184f6feb4bb62d083617d09f0af37a829975a848f7b0a01205907e4238eb00b21510
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
engmohamedsshubber_violencedetectiondataset_path = kagglehub.dataset_download('engmohamedsshubber/violencedetectiondataset')

print('Data source import complete.')

import kagglehub

# Download latest version
path = kagglehub.dataset_download("engmohamedsshubber/violencedetectiondataset")

print("Path to dataset files:", path)

"""**Importing libraries**"""

# Commented out IPython magic to ensure Python compatibility.
import os
import shutil
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow
import keras
from collections import deque
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

from sklearn.model_selection import train_test_split

from keras.layers import *
from keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model

"""**Data Visualization**"""

from IPython.display import HTML
from base64 import b64encode

# To Show a Video in Notebook
def Play_Video(filepath):
    html = ''
    video = open(filepath,'rb').read()
    src = 'data:video/mp4;base64,' + b64encode(video).decode()
    html += '<video width=640 muted controls autoplay loop><source src="%s" type="video/mp4"></video>' % src
    return HTML(html)

# Classes Directories
NonViolnceVideos_Dir = "/kaggle/input/violencedetectiondataset/violence-detection-dataset/non-violent"
ViolnceVideos_Dir = "/kaggle/input/violencedetectiondataset/violence-detection-dataset/violent"

# Retrieve the list of all the video files present in the Class Directory.
NonViolence_files_names_list = os.listdir(NonViolnceVideos_Dir)
Violence_files_names_list = os.listdir(ViolnceVideos_Dir)

# Randomly select a video file from the Classes Directory.
Random_NonViolence_Video = random.choice(NonViolence_files_names_list)
Random_Violence_Video = random.choice(Violence_files_names_list)

"""**Play Random Non Violence Video**"""

Play_Video(f"{NonViolnceVideos_Dir}/{Random_NonViolence_Video}")

"""**Play Random Violence Video**"""

Play_Video(f"{ViolnceVideos_Dir}/{Random_Violence_Video}")

"""**Extracting Frames**"""

# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 16


DATASET_DIR = '/kaggle/input/violencedetectiondataset/violence-detection-dataset'

CLASSES_LIST = ["non-violent","violent"]

def frames_extraction(video_path):

    frames_list = []

    # Read the Video File
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video.
        success, frame = video_reader.read()

        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame
        normalized_frame = resized_frame / 255

        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)


    video_reader.release()

    return frames_list

"""**Creating Our DataSet**"""

def create_dataset():

    features = []
    labels = []
    video_files_paths = []

    # Iterating through all the classes.
    for class_index, class_name in enumerate(CLASSES_LIST):

        print(f'Extracting Data of Class: {class_name}')

        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))

        # Iterate through all the files present in the files list.
        for file_name in files_list:

            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified.
            # So ignore the videos having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their respective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    features = np.asarray(features)
    labels = np.array(labels)

    return features, labels, video_files_paths

# Create the Extracted dataset.
features, labels, video_files_paths = create_dataset()

# Saving the extracted data
np.save("features.npy",features)
np.save("labels.npy",labels)
np.save("video_files_paths.npy",video_files_paths)

"""**One-Hot Encoding**"""

# convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)

"""**Splitting Data To Training &Testing Sets**"""

# Split the Data into Train ( 90% ) and Test Set ( 10% ).
features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.1,
                                                                            shuffle = True, random_state = 42)

print(features_train.shape,labels_train.shape )
print(features_test.shape, labels_test.shape)

"""**Importing MobileNet**"""

from keras.applications.mobilenet_v2 import MobileNetV2

mobilenet = MobileNetV2( include_top=False , weights="imagenet")

"""**Fine-Tuning MobileNet**"""

#Fine-Tuning to make the last 40 layer trainable
mobilenet.trainable=True

for layer in mobilenet.layers[:-40]:
  layer.trainable=False

mobilenet.summary()

"""**Building The Model**"""

def create_model():

    model = Sequential()

    #Specifying Input to match features shape
    model.add(Input(shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))

    # Passing mobilenet in the TimeDistributed layer to handle the sequence
    model.add(TimeDistributed(mobilenet))

    model.add(Dropout(0.25))

    model.add(TimeDistributed(Flatten()))


    lstm_fw = LSTM(units=32)
    lstm_bw = LSTM(units=32, go_backwards = True)

    model.add(Bidirectional(lstm_fw, backward_layer = lstm_bw))

    model.add(Dropout(0.25))

    model.add(Dense(256,activation='relu'))
    model.add(Dropout(0.25))

    model.add(Dense(128,activation='relu'))
    model.add(Dropout(0.25))

    model.add(Dense(64,activation='relu'))
    model.add(Dropout(0.25))

    model.add(Dense(32,activation='relu'))
    model.add(Dropout(0.25))


    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))


    model.summary()

    return model

# Constructing the Model
MoBiLSTM_model = create_model()

"""**Fitting Model**"""

import tensorflow  as tf
# Create Early Stopping Callback to monitor the accuracy
early_stopping_callback = EarlyStopping(monitor = 'val_accuracy', patience = 15, restore_best_weights = True)

# Create ReduceLROnPlateau Callback to reduce overfitting by decreasing learning
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',
                                                  factor=0.6,
                                                  patience=5,
                                                  min_lr=0.0001,
                                                  verbose=1)

# Compiling the model
MoBiLSTM_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ["accuracy"])

# Fitting the model
MobBiLSTM_model_history = MoBiLSTM_model.fit(x = features_train, y = labels_train, epochs = 30, batch_size = 8 ,
                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback,reduce_lr])

"""**Evaluation Of The Model**"""

MoBiLSTM_model.save("violence_detection_model.h5")

def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):

    metric_value_1 = model_training_history.history[metric_name_1]
    metric_value_2 = model_training_history.history[metric_name_2]

    # Get the Epochs Count
    epochs = range(len(metric_value_1))

    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)
    plt.plot(epochs, metric_value_2, 'orange', label = metric_name_2)

    plt.title(str(plot_name))

    plt.legend()

plot_metric(MobBiLSTM_model_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')

plot_metric(MobBiLSTM_model_history, 'accuracy', 'val_accuracy', 'Total Loss vs Total Validation Loss')

"""**Predicting the Test Set**"""

labels_predict = MoBiLSTM_model.predict(features_test)

# Decoding the data to use in Metrics
labels_predict = np.argmax(labels_predict , axis=1)
labels_test_normal = np.argmax(labels_test , axis=1)

labels_test_normal.shape , labels_predict.shape

"""**Accuracy Score**"""

from sklearn.metrics import accuracy_score
AccScore = accuracy_score(labels_predict, labels_test_normal)
print('Accuracy Score is : ', AccScore)

"""**Confusion Matrix**"""

import seaborn as sns
from sklearn.metrics import confusion_matrix

ax= plt.subplot()
cm=confusion_matrix(labels_test_normal, labels_predict)
sns.heatmap(cm, annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix');
ax.xaxis.set_ticklabels(['True', 'False']); ax.yaxis.set_ticklabels(['NonViolence', 'Violence']);

"""**Classification Report**"""

from sklearn.metrics import classification_report

ClassificationReport = classification_report(labels_test_normal,labels_predict)
print('Classification Report is : \n', ClassificationReport)

"""**Prediction Frame By Frame**"""

model = Sequential()
model.save("violence_detection_model.h5")

def predict_frames(video_file_path, output_file_path, SEQUENCE_LENGTH):

    # Read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # VideoWriter to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('m', 'p', '4', 'v'),
                                    video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Store the predicted class in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        ok, frame = video_reader.read()

        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # We Need at Least number of SEQUENCE_LENGTH Frames to perform a prediction.
        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = MoBiLSTM_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        if predicted_class_name == "violent":
            cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 12)
        else:
            cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 12)

        # Write The frame into the disk using the VideoWriter
        video_writer.write(frame)

    video_reader.release()
    video_writer.release()

plt.style.use("default")

# To show Random Frames from the saved output predicted video (output predicted video doesn't show on the notebook but can be downloaded)
def show_pred_frames(pred_video_path):

    plt.figure(figsize=(20,15))

    video_reader = cv2.VideoCapture(pred_video_path)

    # Get the number of frames in the video.
    frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Get Random Frames from the video then Sort it
    random_range = sorted(random.sample(range (SEQUENCE_LENGTH , frames_count ), 12))

    for counter, random_index in enumerate(random_range, 1):

        plt.subplot(5, 4, counter)

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, random_index)

        ok, frame = video_reader.read()

        if not ok:
          break

        frame = cv2.cvtColor(frame , cv2.COLOR_BGR2RGB)

        plt.imshow(frame);ax.figure.set_size_inches(20,20);plt.tight_layout()

    video_reader.release()

# Construct the output video path.
test_videos_directory = 'test_videos'
os.makedirs(test_videos_directory, exist_ok = True)

output_video_file_path = f'{test_videos_directory}/Output-Test-Video.mp4'

# Specifying video to be predicted
input_video_file_path = r"/content/Dallas fan fights Rams fans after lost.mp4"

# Perform Prediction on the Test Video.
predict_frames(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Show random frames from the output video
show_pred_frames(output_video_file_path)

# Play the actual video
Play_Video(input_video_file_path)

"""**Prediction For Video**"""

def predict_video(video_file_path, SEQUENCE_LENGTH):

    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Declare a list to store video frames we will extract.
    frames_list = []

    # Store the predicted class in the video.
    predicted_class_name = ''

    # Get the number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH),1)

    # Iterating the number of times equal to the fixed length of sequence.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        success, frame = video_reader.read()

        if not success:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list
        frames_list.append(normalized_frame)

    # Passing the  pre-processed frames to the model and get the predicted probabilities.
    predicted_labels_probabilities = MoBiLSTM_model.predict(np.expand_dims(frames_list, axis = 0))[0]

    # Get the index of class with highest probability.
    predicted_label = np.argmax(predicted_labels_probabilities)

    # Get the class name using the retrieved index.
    predicted_class_name = CLASSES_LIST[predicted_label]

    # Display the predicted class along with the prediction confidence.
    print(f'Predicted: {predicted_class_name}\nConfidence: {predicted_labels_probabilities[predicted_label]}')

    video_reader.release()

# Specifying video to be predicted
input_video_file_path = "/content/Violence1.MP4"

# Perform Single Prediction on the Test Video.
predict_video(input_video_file_path, SEQUENCE_LENGTH)

# Play the actual video
Play_Video(input_video_file_path)

MoBiLSTM_model.save("violence_detection_model.keras")

pip install gradio tensorflow keras opencv-python numpy

# -*- coding: utf-8 -*-
import gradio as gr
import random
import time
from datetime import datetime # Optional: for timestamps in logs
import os # Needed for checking placeholder file existence

# --- Simulation of Complex Backend Analysis ---
# (Keep the simulate_media_analysis function exactly as before)
def simulate_media_analysis(uploaded_media_data):
    """
    Simulates the analysis of an uploaded image or video.
    Returns a dictionary with assessment results.
    """
    print(f"[{datetime.now()}] Simulating analysis for media type: {type(uploaded_media_data)}")
    processing_time = random.uniform(1.5, 3.5)
    print(f"[{datetime.now()}] Simulating analysis... (approx {processing_time:.1f}s)")
    time.sleep(processing_time)
    scenarios = [
        # ...(keep the scenarios list as before)...
         {
            "assessment": "Likely Safe ‚úÖ",
            "description": "Simulated analysis suggests a calm environment (e.g., park, quiet room) with no immediate visual threats detected.",
            "details": {"objects": ["trees", "bench"], "actions": ["static scene"], "hazards": ["none detected"]},
            "confidence_note": "Confidence: High (Simulated)"
        },
        {
            "assessment": "Caution Advised ü§î",
            "description": "Simulated analysis detected elements requiring attention (e.g., moderate crowding, unusual object placement, unclear activity).",
            "details": {"objects": ["multiple people", "bags"], "actions": ["loitering (simulated)"], "hazards": ["none detected"]},
            "confidence_note": "Confidence: Medium (Simulated)"
        },
        {
            "assessment": "Potential Danger Detected ‚ö†Ô∏è",
            "description": "Simulated analysis indicates potential signs of conflict, hazard, or distress (e.g., aggressive postures, fire-like visual patterns).",
            "details": {"objects": ["people", "vehicle"], "actions": ["agitated movement (simulated)"], "hazards": ["possible conflict"]},
            "confidence_note": "Confidence: Medium-High (Simulated)"
        },
         {
            "assessment": "High Risk Identified üî•", # Example for a specific hazard
            "description": "Simulated analysis strongly suggests a specific hazard (e.g., visual patterns consistent with fire or structural issue). Immediate caution required.",
            "details": {"objects": ["smoke pattern", "obstructed view"], "actions": ["static scene"], "hazards": ["fire/smoke (simulated)"]},
            "confidence_note": "Confidence: High (Simulated)"
        }
    ]
    chosen_scenario = random.choice(scenarios)
    print(f"[{datetime.now()}] Simulation result: {chosen_scenario['assessment']}")
    return chosen_scenario

# --- Chatbot Logic ---
# (Keep the last_analysis_result variable and chatbot_response function exactly as before)
last_analysis_result = None

def chatbot_response(message, history):
    """
    Handles user messages and generates chatbot responses.
    Uses Gradio's multimodal input format.
    """
    global last_analysis_result
    text_input = ""
    media_input = None

    if isinstance(message, dict):
        text_input = message.get('text', '')
        if message.get('files') and isinstance(message['files'], list) and len(message['files']) > 0:
            media_input = message['files'][0]

    print(f"[{datetime.now()}] Received input - Text: '{text_input}', Media detected: {media_input is not None}")

    bot_response = ""

    if media_input:
        yield "Analyzing the uploaded media... This is a simulation and may take a few seconds. ‚è≥"
        analysis_result = simulate_media_analysis(media_input)
        last_analysis_result = analysis_result

        bot_response = f"**Analysis Complete (Simulated)**\n\n"
        bot_response += f"**Assessment:** {analysis_result['assessment']}\n\n"
        bot_response += f"**Description:** {analysis_result['description']}\n\n"
        bot_response += f"**Key Details (Simulated):**\n"
        bot_response += f"  - Objects: {', '.join(analysis_result['details'].get('objects', ['N/A']))}\n"
        bot_response += f"  - Actions: {', '.join(analysis_result['details'].get('actions', ['N/A']))}\n"
        bot_response += f"  - Hazards: {', '.join(analysis_result['details'].get('hazards', ['N/A']))}\n\n"
        bot_response += f"*{analysis_result.get('confidence_note', '')}*\n\n"
        bot_response += ("--- \n*‚ö†Ô∏è **Reminder:** This analysis is **simulated** based on predefined scenarios "
                         "and does not reflect true AI understanding or guarantee accuracy. "
                         "Do not rely on this for real-world safety decisions.*")

    elif text_input:
        # ...(keep the text handling logic as before)...
        text_lower = text_input.lower()
        if "hello" in text_lower or "hi" in text_lower:
            bot_response = ("Hello! I'm a demo chatbot. Upload an image or video, and I'll provide a "
                            "*simulated* safety analysis based on visual content.")
        elif last_analysis_result and ("details" in text_lower or "objects" in text_lower or "actions" in text_lower):
            bot_response = f"Regarding the last analysis:\n"
            bot_response += f"  - Simulated Objects: {', '.join(last_analysis_result['details'].get('objects', ['N/A']))}\n"
            bot_response += f"  - Simulated Actions: {', '.join(last_analysis_result['details'].get('actions', ['N/A']))}\n"
            bot_response += f"  - Simulated Hazards: {', '.join(last_analysis_result['details'].get('hazards', ['N/A']))}"
        elif last_analysis_result and ("assessment" in text_lower or "result" in text_lower or "safe" in text_lower or "danger" in text_lower):
             bot_response = f"The overall simulated assessment for the last media was: **{last_analysis_result['assessment']}**"
        elif "help" in text_lower:
             bot_response = "You can upload an image or video file. After analysis, you can ask simple follow-up questions like 'what were the details?' or 'what was the assessment?'."
        else:
            bot_response = ("I can provide a *simulated* analysis if you upload an image or video. "
                            "You can also ask basic questions about the last analysis performed.")

    else:
        bot_response = "Sorry, I didn't get that. Please type a message or upload an image/video file."

    yield bot_response


# --- Create and Launch the Gradio Chat Interface (Corrected) ---

# Define Chatbot component - Corrected according to warnings
chatbot_component = gr.Chatbot(
    label="Analysis Chat",
    # Removed bubble_full_width (deprecated)
    type="messages", # Added type='messages' as recommended
    height=500
    )

# Define Input component - Stays the same
input_component = gr.MultimodalTextbox(
    file_types=["image", "video"],
    placeholder="Type a message or upload an image/video...",
    label="Your Input"
    )

# Prepare example list conditionally
# Create a dummy file if needed for examples, otherwise disable them.
example_file_exists = False
placeholder_file = "placeholder_image.png" # Define placeholder name
if not os.path.exists(placeholder_file):
    try:
        # Attempt to create a tiny dummy PNG file if PIL is available
        from PIL import Image
        img = Image.new('RGB', (60, 30), color = (210, 210, 210))
        img.save(placeholder_file)
        print(f"Created dummy '{placeholder_file}' for examples.")
        example_file_exists = True
    except ImportError:
        print(f"Warning: PIL not found. Cannot create dummy '{placeholder_file}'. Examples requiring files will be disabled.")
    except Exception as e:
         print(f"Warning: Could not create dummy '{placeholder_file}': {e}. Examples requiring files will be disabled.")
else:
    example_file_exists = True

examples_list = [
        {"text": "hello"},
        {"text": "What was the assessment?"}, # Example follow-up (only works after an analysis)
    ]
if example_file_exists:
    examples_list.insert(0, {'text': 'Check this image', 'files': [placeholder_file]})


# Define the interface using ChatInterface - Corrected arguments
iface = gr.ChatInterface(
    fn=chatbot_response,
    chatbot=chatbot_component,
    textbox=input_component,
    title="Muraqib - ŸÖŸèÿ±ÿßŸÇÿ® ü§ñ",
    description="""**DEMO ONLY - Analysis is SIMULATED**
    Upload an image or video to get a *simulated* assessment of potential safety risks based on visual content.
    You can ask follow-up questions like 'what details were found?' about the last analysis.""",
    theme=gr.themes.Soft(primary_hue="red", secondary_hue="rose"),
    examples=examples_list, # Use the prepared list
    cache_examples=False,
    # Removed unexpected keyword arguments: retry_btn, undo_btn, clear_btn
    # These buttons should appear by default if supported by ChatInterface
)

if __name__ == "__main__":
    print("Launching Gradio Simulated Chatbot...")
    iface.launch(debug=True, share=True) # Set share=True

